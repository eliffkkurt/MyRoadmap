{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'> **Örsan Özener - Özyeğin Üniversitesi - Makine Öğrenmesi Dersi- Yıldız Holding (08.08.2023)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='black'> Neden Accuracy Skor Yerine AUC Skor Tercih Etmeliyiz?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elimizde 7 sample'a ait test datası var. Bu data dengeli bir şekilde dağılmış 4 adet 1 ve 3 adet 0'dan oluşuyor. Classification modellerinde çıktı olarak olasılık değerlerini görürüz, biz bunları class predictionlarına çeviririz. Burada da 2 farklı classification modeli çalıştırmışız. Belirlediğimiz eşik değerine göre de hangisinin 0 hangisinin 1 olduğuna karar vermişiz.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Accuracy aynı olmasına rağmen AUC skoru farklılaşabilir. AUC skoru daha tutarlı sonuç verir.</font>\n",
    "\n",
    "*Tahmin için iki örnekte de modellerin tresholdunu 0.5 aldık.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='brown'> **İlk Örnek**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST DATAMIZ\n",
    "test_y      = [ 1, 0, 0, 1, 0, 1, 1]\n",
    "\n",
    "## İLK MODELİN ÇIKTISI\n",
    "y_pred_prob = [.6,.4,.5,.9,.2,.7,.4]\n",
    "y_pred      = [ 1, 0, 1, 1, 0, 1, 0]\n",
    "\n",
    "# İKİNCİ MODELİN ÇIKTISI\n",
    "y_pred_prob2 = [.4,.3,.4,.8,.2,.6,.4]\n",
    "y_pred2      = [ 0, 0, 0, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**İki modelde de 5 tanesini doğru bilmiş model 2 tanesini yanlış. Dolayısıyla accuracy değerleri aynı.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "Accuracy2: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred))\n",
    "print(\"Accuracy2:\",metrics.accuracy_score(test_y, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 farklı AUC skor hesabı yapıyoruz. İlki class predictionları üzerinden. İkincisi ise olasılıklar üzerinden.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Böyle bir hesaplama şekli yok auc skoru class predictionları üzerinden yapılmaz.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"AUC Score:\", roc_auc_score(test_y, y_pred))\n",
    "# print (\"AUC Score2:\", roc_auc_score(test_y, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Olasılıklar üzerinden yaptığımız AUC skor hesabına baktığımızda ikinci modelin AUC skorunun daha iyi olduğunu görüyoruz. Çünkü ilk modelde 0.5'e çok sınırdan 1 demiş ama ikinci model daha kendinden emin bir model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score prob: 0.875\n",
      "AUC Score prob2: 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"AUC Score prob2:\", roc_auc_score(test_y, y_pred_prob2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='brown'> **İkinci Örnek**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST DATAMIZ\n",
    "test_y      = [ 1, 0, 0, 1, 0, 1, 1]\n",
    "\n",
    "## İLK MODELİN ÇIKTISI\n",
    "y_pred_prob = [.51,.49,.49,.51,.49,.51,.4]\n",
    "y_pred      = [ 1, 0, 0, 1, 0, 1, 0]\n",
    "\n",
    "# İKİNCİ MODELİN ÇIKTISI\n",
    "y_pred_prob2 = [.4,.3,.4,.7,.2,.6,.4]\n",
    "y_pred2      = [ 0, 0, 0, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**İlk modelde 6'sını tutturmuş 1 tanesini tutturmamış. Ama çok sınırdan tutturmuş bunları. Dolayısıyla en ufak bir noisy'den etkilenebilir bu model. İkincisinde ise 5 tane tutturmuşuz 2 tanesini tutturmamışız. Accuracy daha düşük. Daha çok hata yapsa da kafası daha az karışık.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8571428571428571\n",
      "Accuracy2: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred))\n",
    "print(\"Accuracy2:\",metrics.accuracy_score(test_y, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUC skorlarını karşılaştırdığımızda ilk modelin AUC skorunun çok düştüğünü ama ikincisinin hala aynı kaldığını görüyoruz.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score prob: 0.75\n",
      "AUC Score prob2: 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"AUC Score prob2:\", roc_auc_score(test_y, y_pred_prob2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baktığımızda ilkinin accuracy'si daha iyi ama AUC skoru daha kötü. Test datasında bile olsa accuracy'si iyi bile olsa performansı iyi olan değil güvenebileceğimiz-tutarlı bir model arıyoruz. Bu yüzden AUC skoru daha iyi olan modeli tercih etmeliyiz.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>AUC skor prediction için kullandığımız (varsayılan olarak:0.5) eşik değerine göre değişmez fakat accuracy değişir. </font>\n",
    "\n",
    "*İkinci örnekte ilk modelin tresholdunu 0.5'te bıraktık ikinci modelin tresholdunu 0.4 olarak aldık.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model bazlı treshold belirleyebiliriz normalde de. Bu model daha conservative davranıyor bu yüzden tresholdunu aşağı çekiyorum diyebiliriz. Genelde yapılan bir şey değil ama kullanılabilir. Örneğin pozitifleri yakalamam daha öncelikliyse ona ağırlık verecek şekilde tresholdu değiştirebilirim.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST DATAMIZ\n",
    "test_y      = [ 1, 0, 0, 1, 0, 1, 1]\n",
    "\n",
    "## İLK MODELİN ÇIKTISI\n",
    "y_pred_prob = [.51,.49,.49,.51,.49,.51,.4]\n",
    "y_pred      = [ 1, 0, 0, 1, 0, 1, 0]\n",
    "\n",
    "# İKİNCİ MODELİN ÇIKTISI\n",
    "y_pred_prob2 = [.4,.3,.4,.7,.2,.6,.4]\n",
    "y_pred2      = [ 1, 0, 1, 1, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy'lerini aynı yapmış olduk.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8571428571428571\n",
      "Accuracy2: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred))\n",
    "print(\"Accuracy2:\",metrics.accuracy_score(test_y, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fakat olasılıklar değişmediği için AUC skorlar değişmedi. Çünkü AUC skor tresholda bağlı değil. Bütün treshold değerleri için tek bir değer hesaplıyoruz zaten. Fakat accuracy treshold değiştiğinde değişiyor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score prob: 0.75\n",
      "AUC Score prob2: 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"AUC Score prob2:\", roc_auc_score(test_y, y_pred_prob2))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cda24fce70f01a3abc0e1154a9a93142dbc1a65934ebcf00a4afddfd52c450d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv_makine_ogrenmesi': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
