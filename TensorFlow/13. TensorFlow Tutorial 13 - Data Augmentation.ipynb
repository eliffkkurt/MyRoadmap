{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13. TensorFlow Tutorial 13 - Data Augmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvsHwgwpNFcm"
      },
      "source": [
        "<font color='green'> \n",
        "**Youtube - Aladdin Persson Kanalı - TensorFlow 2.0 Beginner Tutorials serisi**\n",
        "    \n",
        "TensorFlow Tutorial 13 - Data Augmentation - Aladdin Persson anlattı.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUDam0uaNOzc"
      },
      "source": [
        "**Video**: [TensorFlow Tutorial 13 - Data Augmentation](https://www.youtube.com/watch?v=8wwfVV7ixyY&list=PLhhyoLH6IjfxVOdVC1P1L5z5azs0XjMsb&index=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE6rBpT4NaJN"
      },
      "source": [
        "### İçindekiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M71bpCZ_NgTd"
      },
      "source": [
        "**Loading Dataset**\n",
        "\n",
        "**Processing the Dataset and Creating Model**\n",
        "\n",
        "*Normalizing*\n",
        "\n",
        "*Data Augmentation*\n",
        "\n",
        "* First Way: Applying it as a function to the train set\n",
        "\n",
        "* Second Way: Write inside the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UbTYHrRNjhC"
      },
      "source": [
        "### <font color=\"blue\"> Giriş</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI6j6D0kNBmB"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9lAUW8IQS5Q"
      },
      "source": [
        "## 1. Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5h8YpqfQSOP"
      },
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    \"cifar10\",\n",
        "    split=[\"train\",\"test\"],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dukua5KzQoMX"
      },
      "source": [
        "## 2. Processing the Dataset and Creating Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuYJRYdyWgSn"
      },
      "source": [
        "### <font color=\"green\">2.1. Normalizing </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDS5Z4xfQlII"
      },
      "source": [
        "def normalize_img(image,label):\n",
        "  return tf.cast(image, tf.float32) / 255.0, label"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sprh8FMmWtpl"
      },
      "source": [
        "### <font color=\"green\">2.2. Data Augmentation </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt2s1BSwRKvz"
      },
      "source": [
        "###### **1. yol: Train setine fonksiyon olarak uygulamak**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_3E-gSZSg2F"
      },
      "source": [
        "Bu yöntemde model eğitilirken paralel olarak cpu üzerinde ilgili batch için data augmentation yapılıyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBrFNAPSWxGw"
      },
      "source": [
        "def augment(image, label):\n",
        "  # image'ı resize etmek istiyoruz diyelim:\n",
        "  new_height = new_width = 32\n",
        "  image = tf.image.resize(image, (new_height, new_width))\n",
        "\n",
        "  # resmi grayscale'e dönüştürmek istiyoruz diyelim:\n",
        "  if tf.random.uniform((), minval=0, maxval=1)<0.1:\n",
        "    image = tf.tile(tf.image.rgb_to_grayscale(image), [1, 1, 3]) \n",
        "  \n",
        "  # resme parlaklığıyla oynamak istiyoruz diyelim:\n",
        "  image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "  image = tf.image.random_contrast(image, lower=0.1, upper=0.2)\n",
        "\n",
        "  # resmi çevirmek istiyoruz diyelim yatay ve dikey olarak:\n",
        "  image = tf.image.random_flip_left_right(image) \n",
        "\n",
        "  # resmi yukarı aşağı çevirmek istediğimizde:\n",
        "  # image = tf.image.random_flip_up_down(image)\n",
        "\n",
        "  return image,label"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp3DbFZbYQSg"
      },
      "source": [
        "* Resimlerimiz şu anda 32 x 32 boyutunda. Ama tüm resimlerimiz bu boyutta olmayabilirdi. Bu durumda resize etmemiz gerekebilirdi.\n",
        "\n",
        "* Resmi grayscale'e dönüştürmek istiyoruz diyelim. Modelimiz hem grayscale üzerinde hem RGB resimler üzerinde tahmin yapacak böylelikle. Fakat büyük ihtimalle tüm resimlerimizi grayscale yapmak istemeyiz, bir ölçüde renk isteriz. Bu yüzden bir olasılıkla convert yapacağız. \n",
        "\n",
        "* `image = tf.image.rgb_to_grayscale(image)`  output channelımız 1 olacak bunu yaptığımız için. Ama bizim modelimiz 3 channel bekliyor. `keras.Input((32, 32, 3))`. Bunu çözmek için bir kanalı kopyalayabiliriz, böylece bir kanalı kopyalayarak üç kanal haline getirebiliriz. Bu üçü birebir aynı olacak. Bunu yapmak için `image = tf.tile(tf.image.rgb_to_grayscale(image), [1, 1, 3])` şeklinde değiştirdik kodu. 1'lerin anlamı o dimensionları kopyalamıyoruz. 3'ün anlamı o dimensionı 3 kez kopyalıyoruz. \n",
        "\n",
        "* Resimleri çevireceğimiz zaman (flip) dikkat etmemiz gerekiyor. Data setini tanımamız gerekiyor. Mesela MNIST'te 9 resmini çevirirsek outputu da değiştirmiş oluruz. \n",
        "\n",
        "* `image = tf.image.random_flip_left_right(image)` Resimlerin %50'sinde flip işlemi yapıyor. Flipping probabilityi kendimiz ayarlayabiliriz bir fonksiyon yazıp."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpmrQ6rnOgU5"
      },
      "source": [
        "Fonksiyonu yazdıktan sonra aşağıda `ds_train = ds_train.map(augment, num_parallel_calls=AUTOTUNE)` koduyla train setimize uyguluyoruz. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2BsAZSxQ_HN"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "ds_train = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
        "ds_train = ds_train.map(augment, num_parallel_calls=AUTOTUNE)  # bunu ekledik\n",
        "ds_train = ds_train.batch(BATCH_SIZE)\n",
        "ds_train = ds_train.prefetch(AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
        "ds_test = ds_test.batch(BATCH_SIZE)\n",
        "ds_test = ds_test.prefetch(AUTOTUNE)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9DgYX7MR7eM"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "     keras.Input((32, 32, 3)),\n",
        "     layers.Conv2D(4, 3, padding=\"same\", activation=\"relu\"),\n",
        "     layers.Conv2D(8, 3, padding=\"same\", activation=\"relu\"),\n",
        "     layers.MaxPooling2D(),\n",
        "     layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
        "     layers.Flatten(),\n",
        "     layers.Dense(64, activation=\"relu\"),\n",
        "     layers.Dense(10)\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MSI12uIVcKF"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(3e-4),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aNe2PkkVsiG",
        "outputId": "9219dcd6-87d3-4f35-fe4f-62d8e208ea14"
      },
      "source": [
        "model.fit(ds_train, epochs=5, verbose=2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 - 55s - loss: 2.3027 - accuracy: 0.0987\n",
            "Epoch 2/5\n",
            "1563/1563 - 10s - loss: 2.3027 - accuracy: 0.0974\n",
            "Epoch 3/5\n",
            "1563/1563 - 10s - loss: 2.3027 - accuracy: 0.0967\n",
            "Epoch 4/5\n",
            "1563/1563 - 10s - loss: 2.3027 - accuracy: 0.0978\n",
            "Epoch 5/5\n",
            "1563/1563 - 10s - loss: 2.3027 - accuracy: 0.0974\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8cd22bb650>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbxqzmINVwyj",
        "outputId": "286fcaa2-aa32-41cc-99ca-59cbe583e260"
      },
      "source": [
        "model.evaluate(ds_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 10ms/step - loss: 2.3040 - accuracy: 0.0868\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.3039538860321045, 0.0868000015616417]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-so2y6OUE7h"
      },
      "source": [
        "###### **2. yol: Modelin içinde yazmak**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnp-YRJSUIRO"
      },
      "source": [
        "Eğer Tensorflow versiyonun 2.3.0 veya daha yeniyse data augmentationın modelin bir parçası olarak yapılması tavsiye ediliyor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsDVE-CnUeJa",
        "outputId": "f81f0670-4fdc-4025-f9c1-56f191490ffc"
      },
      "source": [
        "print(tf. __version__) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFvja-3DUMCi"
      },
      "source": [
        "İlk yöntemde data augmentation model eğitilirken paralel olarak cpu üzerinde yapılıyordu. Bu yöntemde model eğitilirken modelde yapılıyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvDSTjMUUFgA"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "     layers.experimental.preprocessing.Resizing(height=32, width=32),\n",
        "     layers.experimental.preprocessing.RandomFlip(mode=\"horizontal\"),\n",
        "     layers.experimental.preprocessing.RandomContrast(factor=0.1)\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9UynOW9Uy_S"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "ds_train = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
        "ds_train = ds_train.batch(BATCH_SIZE)\n",
        "ds_train = ds_train.prefetch(AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
        "ds_test = ds_test.batch(BATCH_SIZE)\n",
        "ds_test = ds_test.prefetch(AUTOTUNE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xshx1zCSd0Pl"
      },
      "source": [
        "Burada modele ekliyoruz yazdığımız data augmentationı."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjivkD2EU0jW"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "     keras.Input((32, 32, 3)),\n",
        "     data_augmentation, # bunu ekledik\n",
        "     layers.Conv2D(4, 3, padding=\"same\", activation=\"relu\"),\n",
        "     layers.Conv2D(8, 3, padding=\"same\", activation=\"relu\"),\n",
        "     layers.MaxPooling2D(),\n",
        "     layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
        "     layers.Flatten(),\n",
        "     layers.Dense(64, activation=\"relu\"),\n",
        "     layers.Dense(10)\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzlVR2BtU5Uh"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(3e-4),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVKorbiKU6Cx",
        "outputId": "38449d85-8618-4c9d-a75c-650d0bae0243"
      },
      "source": [
        "model.fit(ds_train, epochs=5, verbose=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 - 21s - loss: 1.7369 - accuracy: 0.3802\n",
            "Epoch 2/5\n",
            "1563/1563 - 8s - loss: 1.4008 - accuracy: 0.5072\n",
            "Epoch 3/5\n",
            "1563/1563 - 8s - loss: 1.2751 - accuracy: 0.5531\n",
            "Epoch 4/5\n",
            "1563/1563 - 8s - loss: 1.1909 - accuracy: 0.5821\n",
            "Epoch 5/5\n",
            "1563/1563 - 8s - loss: 1.1296 - accuracy: 0.6053\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0c30247150>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMNcps2rU8DX",
        "outputId": "1733f08a-35c3-478a-b706-9edbaf650fd3"
      },
      "source": [
        "model.evaluate(ds_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1371 - accuracy: 0.6007\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1371263265609741, 0.6007000207901001]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk4SlNxjZ_Re"
      },
      "source": [
        "İlkinde daha fazla data augmentation uygulamamıza rağmen accuracy daha düşük çıktı. Daha fazla data augmentation uygulamak modelin overfit yapmasına ve modelin öğrenmesinin zorlaşmasına sebep oluyor."
      ]
    }
  ]
}